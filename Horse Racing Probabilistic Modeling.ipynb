{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c27ca42-05a3-44d5-84a3-f1e36c202045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing & Model\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, cross_val_score\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    r2_score,\n",
    "    log_loss,\n",
    "    brier_score_loss\n",
    ")\n",
    "\n",
    "# Statistical Tests\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2851a-1138-440d-8aa1-37ca9c87420e",
   "metadata": {},
   "source": [
    "#### 1. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff77553e-00bc-4123-a228-a5d468364601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test data\n",
    "train = pd.read_csv(\"trainData.csv\")\n",
    "test  = pd.read_csv(\"testData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd94e178-ac37-44fd-ae06-e972f0cae2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with invalid time\n",
    "train = train[train.timeSecs != 0].copy()\n",
    "test  = test[test.timeSecs  != 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d98ebb-0c5a-4df5-b074-a73ed3556989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute target variable: Speed = distance / time\n",
    "train[\"Speed_Target\"] = train.distanceYards / train.timeSecs\n",
    "test[\"Speed_Target\"]  = test.distanceYards  / test.timeSecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e743a28-1e03-4674-b958-92d13d16e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NA for critical columns in training\n",
    "clean_cols = [\n",
    "    \"Speed_PreviousRun\",\"Speed_2ndPreviousRun\",\"NMFPLTO\",\n",
    "    \"MarketOdds_PreviousRun\",\"MarketOdds_2ndPreviousRun\",\n",
    "    \"TrainerRating\",\"JockeyRating\",\"daysSinceLastRun\",\n",
    "    \"SireRating\", \"DamsireRating\", \"meanRunners\"\n",
    "]\n",
    "train = train.dropna(subset=clean_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a5a3bf-4810-4e69-9415-5fe767b7077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute test missing values using train's Going-median\n",
    "impute_cols = [\n",
    "    \"Speed_PreviousRun\",\"Speed_2ndPreviousRun\",\"NMFPLTO\",\n",
    "    \"MarketOdds_PreviousRun\",\"MarketOdds_2ndPreviousRun\",\n",
    "    \"TrainerRating\",\"JockeyRating\",\"daysSinceLastRun\",\n",
    "    \"SireRating\",\"meanRunners\"\n",
    "]\n",
    "\n",
    "# exclude 'DamsireRating'\n",
    "going_median = {\n",
    "    (g, c): train[train.Going == g][c].median()\n",
    "    for g in train.Going.unique()\n",
    "    for c in impute_cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f23449-6a6a-4531-87f5-3a9cbe5877a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_by_going(df):\n",
    "    df = df.copy()\n",
    "    for g in df.Going.unique():\n",
    "        for c in impute_cols:\n",
    "            m = going_median.get((g, c))\n",
    "            if pd.notna(m):\n",
    "                mask = (df.Going == g) & df[c].isna()\n",
    "                df.loc[mask, c] = m\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5526d0-ac49-4e9d-951b-bfc1a8415f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fill_by_going(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30f76b-0410-4974-ac28-119a6c3bde2b",
   "metadata": {},
   "source": [
    "#### 2. Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ca7f29-2b31-4aab-8c24-70ebb777ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_race_relative(df):\n",
    "    df = df.copy()\n",
    "    grp = df.groupby(\"Race_ID\")\n",
    "    df[\"Rel_SpeedPrev\"]    = df.Speed_PreviousRun - grp.Speed_PreviousRun.transform(\"mean\")\n",
    "    df[\"Odds_Rank\"]        = grp.MarketOdds_PreviousRun.rank(\"min\")\n",
    "    df[\"Rel_JockeyRating\"] = df.JockeyRating - grp.JockeyRating.transform(\"mean\")\n",
    "    df[\"Rel_TrainerRating\"] = df.TrainerRating - grp.TrainerRating.transform(\"mean\")\n",
    "    df[\"Rel_DaysSinceRun\"] = df.daysSinceLastRun - grp.daysSinceLastRun.transform(\"mean\")\n",
    "    df[\"Speed_Trend\"]      = df.Speed_PreviousRun - df.Speed_2ndPreviousRun\n",
    "    df[\"InvOdds\"]          = 1 / df.MarketOdds_PreviousRun\n",
    "    df[\"Rel_InvOdds\"]      = df.InvOdds - grp.InvOdds.transform(\"mean\")\n",
    "    df[\"Rel_NMFPLTO\"]      = df.NMFPLTO - grp.NMFPLTO.transform(\"mean\")\n",
    "    df[\"Age_Rank\"]         = grp.Age.rank(\"dense\")\n",
    "    df[\"Speed_SD\"]         = grp.Speed_PreviousRun.transform(\"std\")\n",
    "    df[\"Odds_SD\"]          = grp.MarketOdds_PreviousRun.transform(\"std\")\n",
    "    df[\"Prize_Rank\"]       = grp.Prize.rank(\"dense\", ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9573c6c2-bd76-4cdf-98cf-226fcb2fe60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_race_relative(train)\n",
    "test  = add_race_relative(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313094b-9a91-40a7-8bab-e68511d36545",
   "metadata": {},
   "source": [
    "#### 3. Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "308539f7-9239-4e20-a16e-a92b769f505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feats = ['Course', 'distanceYards', 'Prize', 'Going', 'Runners', 'Age',\n",
    "    'Speed_PreviousRun', 'Speed_2ndPreviousRun', 'NMFPLTO', 'MarketOdds_PreviousRun',\n",
    "    'MarketOdds_2ndPreviousRun', 'TrainerRating', 'JockeyRating',\n",
    "    'daysSinceLastRun', 'SireRating', 'DamsireRating', 'meanRunners']\n",
    "new_feats = [\n",
    "    \"Rel_SpeedPrev\",\"Odds_Rank\",\"Rel_JockeyRating\",\"Rel_TrainerRating\",\"Rel_DaysSinceRun\",\n",
    "    \"Speed_Trend\",\"InvOdds\",\"Rel_InvOdds\",\"Rel_NMFPLTO\",\n",
    "    \"Age_Rank\", \"Speed_SD\",\"Odds_SD\",\"Prize_Rank\"\n",
    "]\n",
    "features = base_feats + new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56396ab2-3f91-4d75-a372-815424dba339",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Course','Going']\n",
    "numeric_cols = [f for f in features if f not in cat_cols]\n",
    "\n",
    "# Yeo-Johnson transform for skewed features\n",
    "skewness = train[numeric_cols].skew().sort_values()\n",
    "to_power_transform = skewness[skewness.abs() > 1].index.tolist()\n",
    "to_standard_scale = [f for f in numeric_cols  if f not in to_power_transform]\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "scaler = StandardScaler()\n",
    "train[to_power_transform] = pt.fit_transform(train[to_power_transform])\n",
    "test[to_power_transform]  = pt.transform(test[to_power_transform])\n",
    "train[to_standard_scale] = scaler.fit_transform(train[to_standard_scale])\n",
    "test[to_standard_scale]  = scaler.transform(test[to_standard_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ee1c2b9-a6c2-4638-98cc-98491f125325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical features\n",
    "train_ohe = pd.get_dummies(train[cat_cols], drop_first=True)\n",
    "test_ohe  = pd.get_dummies(test[cat_cols],  drop_first=True)\n",
    "test_ohe  = test_ohe.reindex(columns=train_ohe.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2dd30f-83ce-4a0b-ba3a-919a63682274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature matrix\n",
    "numeric_feats = to_power_transform + to_standard_scale\n",
    "X_full = pd.concat([train[numeric_feats], train_ohe], axis=1)\n",
    "X_test = pd.concat([test[numeric_feats],  test_ohe],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3dfb960-e24e-4708-9f01-ba21546fd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop high-correlation features\n",
    "corr_matrix = X_full.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "X_full.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "X_test.drop(columns=to_drop, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985f9b9-fc2f-4d9d-9270-e096d8e738dc",
   "metadata": {},
   "source": [
    "#### 4. Transform Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62be2634-b37d-4952-af5b-7145b9d32578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_y = PowerTransformer(method=\"yeo-johnson\")\n",
    "y_full = pt_y.fit_transform(train[[\"Speed_Target\"]]).flatten()\n",
    "y_test = pt_y.transform(test[[\"Speed_Target\"]]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f033e4c-68cd-48a8-a214-dad352e3eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_full shape: (51574, 91)\n",
      "X_test shape: (11246, 91)\n",
      "Feature count: 91\n"
     ]
    }
   ],
   "source": [
    "print(\"X_full shape:\",  X_full.shape)\n",
    "print(\"X_test shape:\",  X_test.shape)\n",
    "print(\"Feature count:\", X_full.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50695fa6-859d-4b9b-a43f-7e1023a38d42",
   "metadata": {},
   "source": [
    "#### 5. Feature Selection and OLS Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a72a8933-e2f3-46b0-8c56-592828cc6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped cross-validation split\n",
    "if isinstance(y_full, pd.Series):\n",
    "    y_full_array = y_full.values\n",
    "else:\n",
    "    y_full_array = y_full\n",
    "\n",
    "gkf = GroupKFold(n_splits=3)\n",
    "tr_idx, val_idx = next(gkf.split(X_full, y_full_array, train[\"Race_ID\"]))\n",
    "\n",
    "X_train_fold = X_full.iloc[tr_idx]\n",
    "X_val_fold   = X_full.iloc[val_idx]\n",
    "y_train_fold = y_full_array[tr_idx]\n",
    "y_val_fold   = y_full_array[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60def65d-205a-4e2b-b2be-8220286b97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base model and calculate permutation importance\n",
    "base = LinearRegression()\n",
    "base.fit(X_train_fold, y_train_fold)\n",
    "perm = permutation_importance(\n",
    "    base, X_val_fold, y_val_fold,\n",
    "    n_repeats=5, random_state=1,\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "perm_importances = pd.Series(perm.importances_mean, index=X_full.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d784e-1912-455a-aa80-eb01f5615d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for top-k features\n",
    "results = []\n",
    "topk_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 91]\n",
    "for k in topk_list:\n",
    "    feats_k = perm_importances.nlargest(k).index.tolist()\n",
    "    X_sub   = X_full[feats_k]\n",
    "    model   = LinearRegression()\n",
    "    scores  = -cross_val_score(\n",
    "        model, X_sub, y_full_array,\n",
    "        groups=train[\"Race_ID\"], cv=gkf,\n",
    "        scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n",
    "    )\n",
    "    results.append({\"Top_K\": k, \"RMSE_Mean\": scores.mean(), \"RMSE_SD\": scores.std()})\n",
    "df_topk = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10c6cb-7b1f-429d-9ca5-ef13cb79f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = go.Figure()\n",
    "\n",
    "# Mean RMSE line with markers\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=df_topk[\"Top_K\"],\n",
    "    y=df_topk[\"RMSE_Mean\"],\n",
    "    mode=\"lines+markers\",\n",
    "    name=\"Mean RMSE\"\n",
    "))\n",
    "\n",
    "# Error bars (Standard Deviation)\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=df_topk[\"Top_K\"],\n",
    "    y=df_topk[\"RMSE_Mean\"],\n",
    "    mode=\"markers\",\n",
    "    error_y=dict(type=\"data\", array=df_topk[\"RMSE_SD\"], visible=True),\n",
    "    name=\"SD\",\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig1.update_layout(\n",
    "    height=500,\n",
    "    width=600,\n",
    "    template=\"plotly_white\",\n",
    "    title=\"CV RMSE vs. Number of Top-K Features\",\n",
    "    xaxis_title=\"Top-K Features\",\n",
    "    yaxis_title=\"CV RMSE\"\n",
    ")\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e815ec-3d4c-4a2a-be89-746198ef7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare top 20 permutation importances\n",
    "top20_df = perm_importances.nlargest(20).reset_index()\n",
    "top20_df.columns = [\"Feature\", \"Importance\"]\n",
    "\n",
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    x=top20_df[\"Importance\"],\n",
    "    y=top20_df[\"Feature\"],\n",
    "    orientation='h',\n",
    "    name=\"Feature Importance\"\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    height=600,\n",
    "    width=600,\n",
    "    template=\"plotly_white\",\n",
    "    title=\"Top 20 Features by Permutation Importance\",\n",
    "    xaxis_title=\"Importance\",\n",
    "    yaxis=dict(categoryorder=\"total ascending\")\n",
    ")\n",
    "\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e923cab-1156-40dc-beca-06b45c18eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Top-K features from permutation importance\n",
    "best_k = 70\n",
    "best_features = perm_importances.nlargest(best_k).index.tolist()\n",
    "X_selected = X_full[best_features]\n",
    "\n",
    "# Rank all features by importance (1 = most important)\n",
    "feature_ranks = perm_importances.rank(ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c8511-119b-481b-aa06-92acaae12a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a summary DataFrame to examine retention and importance of engineered relative features\n",
    "df_relative_retained = pd.DataFrame({\n",
    "    \"Feature\": new_feats,\n",
    "    \"Retained_in_TopK\": [f in best_features for f in new_feats],\n",
    "    \"Importance_Rank\": [feature_ranks[f] if f in best_features else None for f in new_feats]\n",
    "})\n",
    "\n",
    "# Sort: retained features first, then by importance rank\n",
    "df_relative_retained = df_relative_retained.sort_values(\n",
    "    by=[\"Retained_in_TopK\", \"Importance_Rank\"],\n",
    "    ascending=[False, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_relative_retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1d37b-9df3-40fc-a7c7-1a03421c986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_selected, y_full, test_size=0.2, random_state=0)\n",
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "y_pred = ols.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce259f9e-5c45-47a4-9195-16d1992d90dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae  = mean_absolute_error(y_val, y_pred)\n",
    "r2   = r2_score(y_val, y_pred)\n",
    "print(\"Model (OLS with Top-K features) |   RMSE    |    MAE    |    R²\")\n",
    "print(f\"OLS Top-{best_k:<2}              | {rmse:8.4f} | {mae:8.4f} | {r2:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba93e1-8db8-4c95-91a6-66499e9dce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot of Predictions vs True values\n",
    "df_eval = pd.DataFrame({\"Actual\": y_val, \"Predicted\": y_pred})\n",
    "fig = px.scatter(df_eval, x=\"Actual\", y=\"Predicted\", opacity=0.6, trendline=\"ols\",\n",
    "                 title=\"OLS Predicted vs Actual (Validation Set)\",\n",
    "                 labels={\"Actual\": \"True Speed\", \"Predicted\": \"Predicted Speed\"})\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467db0b-2d87-49c6-9ec9-0d4c08124f3d",
   "metadata": {},
   "source": [
    "#### 6. OLS Model Assumption Checks (Normality, Homoscedasticity, Autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6c5f7-f7b5-4095-9a20-2edee347dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = y_val - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e231942-64db-495c-a99e-0f0d84cf590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1-row, 3-column subplot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Q-Q Plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0])\n",
    "axes[0].set_title(\"Q-Q Plot of Residuals\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Histogram + KDE\n",
    "sns.histplot(residuals, kde=True, ax=axes[1], color='skyblue')\n",
    "axes[1].set_title(\"Histogram of Residuals\")\n",
    "axes[1].set_xlabel(\"Residuals\")\n",
    "\n",
    "# Residuals vs Fitted Values\n",
    "axes[2].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[2].axhline(0, color='red', linestyle='--')\n",
    "axes[2].set_title(\"Residuals vs. Fitted Values\")\n",
    "axes[2].set_xlabel(\"Fitted Values\")\n",
    "axes[2].set_ylabel(\"Residuals\")\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51d865-7a91-4f35-b60c-f75fc79a498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durbin-Watson test for autocorrelation\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson statistic: {dw_stat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec8905-89e2-472f-977b-57027c3504a0",
   "metadata": {},
   "source": [
    "#### 7. Inverse Transform and Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af61620-75fd-419e-aa7a-550e753a171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train OLS using the full training data\n",
    "ols_final = LinearRegression()\n",
    "ols_final.fit(X_full[best_features], y_full)\n",
    "\n",
    "# Predict using the trained OLS model\n",
    "y_pred_ols = ols_final.predict(X_test[best_features])\n",
    "\n",
    "# Inverse-transform predictions\n",
    "y_pred_original = pt_y.inverse_transform(y_pred_ols.reshape(-1, 1)).flatten()\n",
    "\n",
    "# True test labels\n",
    "y_true = test[\"Speed_Target\"]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rmse_ols = root_mean_squared_error(y_true, y_pred_original)\n",
    "mae_ols  = mean_absolute_error(y_true, y_pred_original)\n",
    "r2_ols   = r2_score(y_true, y_pred_original)\n",
    "\n",
    "print(\"Model    |   RMSE    |    MAE    |    R²\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"OLS      | {rmse_ols:.4f}  | {mae_ols:.4f}  | {r2_ols:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e930f2f-1620-4e91-a7a1-d2b4f2f51599",
   "metadata": {},
   "source": [
    "#### 8. Monte Carlo Simulation for Win Probability and Scoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32608e53-5992-4152-9779-746510aa487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prediction to test sets\n",
    "test[\"Pred_OLS\"]  = ols_final.predict(X_test[best_features])\n",
    "\n",
    "# Estimate global residual standard deviation\n",
    "sigma_global_ols = (train[\"Speed_Target\"]).std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206de4f-fa32-459e-859a-c70f2c9a70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo sampling function\n",
    "def monte_carlo_global(df, speed_col, sigma, sims=50000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    out = []\n",
    "    for rid, grp in df.groupby(\"Race_ID\", sort=False):\n",
    "        m = grp[speed_col].values\n",
    "        z = rng.standard_normal((sims, len(m)))\n",
    "        sims_arr = m + sigma * z\n",
    "        idx = np.argmax(sims_arr, axis=1)\n",
    "        counts = np.bincount(idx, minlength=len(m))\n",
    "        out.append(pd.Series(counts / counts.sum(), index=grp.index))\n",
    "    return pd.concat(out).sort_index()\n",
    "\n",
    "test[\"Win_OLS_Global\"] = monte_carlo_global(test, \"Pred_OLS\", sigma_global_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9e755-eeb7-4120-a8b9-eb3be5e08671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Log Loss and Brier Score\n",
    "y_true_bin = (test[\"Position\"] == 1).astype(int)\n",
    "logloss = log_loss(y_true_bin, test[\"Win_OLS_Global\"], labels=[0, 1])\n",
    "brier   = brier_score_loss(y_true_bin, test[\"Win_OLS_Global\"])\n",
    "\n",
    "print(f\"OLS Global → Log Loss: {logloss:.6f}, Brier Score: {brier:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750937b-9e60-4ee0-bbb1-e3a38cdc6ec4",
   "metadata": {},
   "source": [
    "#### 9. Market vs Model Comparison (Accuracy, Spearman ρ, Random Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23174234-0a09-46f4-9dc9-844f35df2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute implied probabilities from Betfair SP\n",
    "test[\"Imp_Raw\"] = 1 / test[\"betfairSP\"]\n",
    "test[\"Imp_Prob\"] = test.groupby(\"Race_ID\")[\"Imp_Raw\"].transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1752b-2629-47be-b004-3c5a5402ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation between market and model\n",
    "rhos = []\n",
    "for _, grp in test.groupby(\"Race_ID\"):\n",
    "    rho, _ = spearmanr(grp[\"Imp_Prob\"], grp[\"Win_OLS_Global\"])\n",
    "    rhos.append(rho)\n",
    "median_rho = np.nanmedian(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f0d67-ca18-4ea6-b782-91266a56307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking and accuracy check\n",
    "for col in [\"Imp_Prob\", \"Win_OLS_Global\"]:\n",
    "    test[f\"{col}_Rank\"] = (\n",
    "        test.groupby(\"Race_ID\")[col]\n",
    "            .rank(method=\"first\", ascending=False)\n",
    "            .astype(int)\n",
    "    )\n",
    "\n",
    "test[\"Correct_Market\"] = test[\"Imp_Prob_Rank\"] == test[\"Position\"]\n",
    "test[\"Correct_Model\"]  = test[\"Win_OLS_Global_Rank\"] == test[\"Position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be214e-9871-4283-a668-89e91c740368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horse-level accuracy\n",
    "acc_market = test[\"Correct_Market\"].mean()\n",
    "acc_model  = test[\"Correct_Model\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5a29c-7184-4889-9244-4329a500b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champion prediction accuracy\n",
    "champ_results = []\n",
    "for _, grp in test.groupby(\"Race_ID\"):\n",
    "    market_won = grp.loc[grp[\"Imp_Prob_Rank\"].idxmin()][\"Position\"] == 1\n",
    "    model_won  = grp.loc[grp[\"Win_OLS_Global_Rank\"].idxmin()][\"Position\"] == 1\n",
    "    champ_results.append((market_won, model_won))\n",
    "\n",
    "champ_results = np.array(champ_results)\n",
    "champ_acc_market = champ_results[:, 0].mean()\n",
    "champ_acc_model  = champ_results[:, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46beb4-45cd-4ba6-8d6f-c8e988039bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random baseline\n",
    "avg_runners = test.groupby(\"Race_ID\")[\"Horse\"].count().mean()\n",
    "random_baseline = 1 / avg_runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e7dce-62bd-4270-81d9-dd18472fc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"Median Spearman ρ (Market vs Model):\", round(median_rho, 4))\n",
    "print(f\"Horse-level Accuracy – Market: {acc_market:.2%}, Model: {acc_model:.2%}\")\n",
    "print(f\"Champion Accuracy – Market: {champ_acc_market:.2%}, Model: {champ_acc_model:.2%}, Random: {random_baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7a0ee-c1f1-441d-95db-74ca96df90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of champion prediction accuracy\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Market', x=[\"Champion Accuracy\"], y=[champ_acc_market]),\n",
    "    go.Bar(name='OLS Model', x=[\"Champion Accuracy\"], y=[champ_acc_model]),\n",
    "    go.Bar(name='Random Guess', x=[\"Champion Accuracy\"], y=[random_baseline])\n",
    "])\n",
    "fig.update_layout(\n",
    "    title=\"Champion Prediction Accuracy Comparison\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    barmode='group',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a43d8f-2841-47c8-995d-9241ffcad161",
   "metadata": {},
   "source": [
    "#### 10. Submission Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5edad-3a14-4a12-891d-52ffe19ab0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"Race_ID\": test[\"Race_ID\"],\n",
    "    \"Horse\": test[\"Horse\"],\n",
    "    \"Predicted_Probability\": test[\"Win_OLS_Global\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82423ff-43e3-438a-aaf7-4711a51044f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that probabilities sum to 1 per race\n",
    "race_sums = submission.groupby(\"Race_ID\")[\"Predicted_Probability\"].sum()\n",
    "race_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fbf9f-6b9a-46b9-a8ee-231c1f01a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "submission.to_csv(\"predicted_probabilities.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
